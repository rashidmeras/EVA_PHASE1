{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EVA_S4_Assignment1_c.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rashidmeras/EVA_PHASE1/blob/master/EVA_S4_Assignment1_c.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNyZv-Ec52ot",
        "colab_type": "text"
      },
      "source": [
        "# EVA (M6) Session4 Assignment: Proposal3\n",
        "\n",
        "Objective:\n",
        "\n",
        "> Using the network defined in Proposal2 explore different techniques and reduce the total number of parameters such that it is not more than 15K and the validation accuracy is above 99.2%.\n",
        "\n",
        "\n",
        "*So let's Start!!*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFIK3KZEHAu-",
        "colab_type": "text"
      },
      "source": [
        "Install the keras API library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3m3w1Cw49Zkt",
        "colab_type": "code",
        "outputId": "72b4bee0-4e57-427e-d047-d99f09f450b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glODJyl3JLIE",
        "colab_type": "text"
      },
      "source": [
        "From Keras API library following APIs are needed to create a DNN:\n",
        "\n",
        "* The sequential API allows to create models layer-by-layer\n",
        "* The Flatten API flattens the input. Does not affect the batch size.\n",
        "* The Convolution2D API creates a convolution kernel that is convolved with the layer input.\n",
        "* The np_utils API is used to convert a class vector (integers) to binary class matrix.\n",
        "* Finally import the MNSIT dataset from Keras\n",
        "\n",
        "MNIST has a training set of 60,000 examples, and a test set of 10,000 examples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eso6UHE080D4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten, Add, BatchNormalization\n",
        "from keras.layers import Convolution2D, MaxPooling2D\n",
        "from keras.utils import np_utils\n",
        "\n",
        "from keras.datasets import mnist"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zByEi95J86RD",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "Load the the data, shuffled and split between train and test sets.\n",
        "\n",
        ">The MNIST dataset consists of pair, “handwritten digit image” and “label”. Digit ranges from 0 to 9, meaning 10 patterns in total.\n",
        "\n",
        "* handwritten digit image (X_train): This is gray scale image with size 28 x 28 pixel.\n",
        "* label (y_train): This is actual digit number this handwritten digit image represents. It is the numbers between including 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7eRM0QWN83PV",
        "colab_type": "code",
        "outputId": "ef224530-f1dc-42c6-f41f-0da6322ceef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 15s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nYa6Dh5WJ1-o",
        "colab_type": "text"
      },
      "source": [
        "Matplotlib is a Python 2D plotting library & PyPlot is a shell-like interface to Matplotlib\n",
        "\n",
        "Display the data in X_train[0] array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a4Be72j8-ZC",
        "colab_type": "code",
        "outputId": "75f8e1e9-daff-41a9-a969-bda0cbc9dc0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "print (X_train.shape)\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.imshow(X_train[1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fe96f447048>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRfiBoAg/EBThB4Ii/EBQhB8I6v8AG8x2aarNGp8AAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tuJ7CUzrKA8U",
        "colab_type": "text"
      },
      "source": [
        "Flatten 28x28 images to a 28*28=784 vector for each image.\n",
        "\n",
        "> The images in the dataset are of 28*28 dimensions which is difficult to accommodate in a simple multilayer neural network. Therefore we need to convert the images into a single dimension where each image contains 784-pixel data using the reshape() function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkmprriw9AnZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.reshape(X_train.shape[0], 28, 28,1)\n",
        "X_test = X_test.reshape(X_test.shape[0], 28, 28,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciBpETfIKHld",
        "colab_type": "text"
      },
      "source": [
        "The pixel values in the images are in the range of 0 - 255 and in this step we reduce this range even further and normalize it between 0 and 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2m4YS4E9CRh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train /= 255\n",
        "X_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWcDTj4QKN3r",
        "colab_type": "text"
      },
      "source": [
        "label : This is actual digit number this handwritten digit image represents. It is the numbers between including 0 to 9."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mn0vAYD9DvB",
        "colab_type": "code",
        "outputId": "68e83118-59ac-4ae2-f43a-6bae7f3f67d7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "y_train[:10]"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, 1, 9, 2, 1, 3, 1, 4], dtype=uint8)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NLywadlZKTIj",
        "colab_type": "text"
      },
      "source": [
        "Convert class vectors to binary class matrices:\n",
        "\n",
        "> As we can see from above, the output of y_train is an integer from 0 to 9. We need to perform one-hot encoding of the class labels for getting a vector of class integers into a binary matrix. We need to do this to do a “binarization” of the category and so that we can include it as a feature to train the neural network.\n",
        "\n",
        "We can use the built in np_utils.to_categorical() helper function in keras to do this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZG8JiXR39FHC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert 1-dimensional class arrays to 10-dimensional class matrices\n",
        "Y_train = np_utils.to_categorical(y_train, 10)\n",
        "Y_test = np_utils.to_categorical(y_test, 10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOTmTwTNKiWh",
        "colab_type": "text"
      },
      "source": [
        "Print the Y_train array after binarization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fYlFRvKS9HMB",
        "colab_type": "code",
        "outputId": "fc7c0a89-8c3f-455f-d7a1-50b17759d8df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "Y_train[:10]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LewGS1xoKnyr",
        "colab_type": "text"
      },
      "source": [
        "##Re-analyse Network Type3\n",
        "\n",
        "\n",
        "![Proposed Network Archtecture](https://rashidmeras.github.io/images/eva/S4_Proposal3_Fig1.png)\n",
        "\n",
        "The Figure1 above shows the network architecteure that we have established so far. We named this network as Network Type3. We will re-examine this network in this section and explore its characteristics.\n",
        "\n",
        "As shown in the Figure we can divide this network into different blocks, namely Block1, Block2 and Transitional Block. Layer1-4 can be grouped into Block1, Layer6-9 can be grouped into Block2 and the remaining Max-Pooling layer5 can be called as Tranisitional Block. Also we can see the total number of channels in each layer.\n",
        "\n",
        "One of the merits of doing this segmentation of the network is that now we can play around with channel size of each block and observe how the network performs. In this *Proposal3*  we will see how changing the channel size in Block1 and Block2 we can redesign the network Type3 and create a new *Network Type4*\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVDdx3rqg1Vz",
        "colab_type": "text"
      },
      "source": [
        "##Network Type4\n",
        "\n",
        "\n",
        "![Proposed Network Archtecture](https://rashidmeras.github.io/images/eva/S4_Proposal3_Fig2.png)\n",
        "\n",
        "\n",
        "Compared to the *Proposal2* network architecture, following are the changes proposed here in Network Type4:\n",
        "\n",
        "> 1. At each layer of we perform Convolution followed by Batch Normalization and then Drop-Out as shown in Figure2.\n",
        "> 2. The channel size are reduced only per block i.e only in Block1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLOycaZCqHA2",
        "colab_type": "text"
      },
      "source": [
        "Let's fix a random seed=**990** for reproducibility!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lT1QAY8ZqH_-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# fix random seed for reproducibility\n",
        "seed = 990\n",
        "np.random.seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhmFT7KTqQOt",
        "colab_type": "text"
      },
      "source": [
        "Implement Network Type4:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osKqT73Q9JJB",
        "colab_type": "code",
        "outputId": "ae7df791-2481-4f14-85ff-36499e6ce634",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1244
        }
      },
      "source": [
        "from keras.layers import Activation\n",
        "model = Sequential()\n",
        " \n",
        "#Layer1: Cov->BN->DO i/p:|28x28x1|Conv(3x3x1)x8| o/p:|26x26x8|\n",
        "model.add(Convolution2D(8, 3, 3, activation='relu', use_bias=False, input_shape=(28,28,1))) #26\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer2: Cov->BN->DO i/p:|26x26x8|Conv(3x3x8)x4| o/p:|24x24x4|\n",
        "model.add(Convolution2D(4, 3, 3, activation='relu', use_bias=False)) #24\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer3: Cov->BN->DO i/p:|24x24x4|Conv(3x3x4)x4| o/p:|22x22x4|\n",
        "model.add(Convolution2D(4, 3, 3, activation='relu', use_bias=False)) #22\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer4: Cov->BN->DO i/p:|22x22x4|Conv(1x1x4)x3| o/p:|22x22x3|\n",
        "model.add(Convolution2D(3, 1, 1, activation='relu', use_bias=False)) #22\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer5: Max-Pooling layer\n",
        "model.add(MaxPooling2D(pool_size=(2, 2))) #11\n",
        "\n",
        "#Layer6: Cov->BN->DO i/p:|11x11x3|Conv(3x3x3)x16| o/p:|9x9x16|\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False)) #9\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer7: Cov->BN->DO i/p:|9x9x16|Conv(3x3x16)x16| o/p:|7x7x16|\n",
        "model.add(Convolution2D(16, 3, 3, activation='relu', use_bias=False)) #7\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer8: Cov->BN->DO i/p:|7x7x16|Conv(7x7x16)x10| o/p:|1x1x10|\n",
        "model.add(Convolution2D(10, 7, 7, use_bias=False)) #1\n",
        "model.add(BatchNormalization())\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "#Layer9: Flatten & activation\n",
        "model.add(Flatten())\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "#Print model summary\n",
        "model.summary()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(8, (3, 3), activation=\"relu\", use_bias=False, input_shape=(28, 28, 1...)`\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:10: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "  # Remove the CWD from sys.path while we load stuff.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:15: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(4, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "  from ipykernel import kernelapp as app\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:20: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(3, (1, 1), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:33: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (3, 3), activation=\"relu\", use_bias=False)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:38: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(10, (7, 7), use_bias=False)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 26, 26, 8)         72        \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 26, 26, 8)         32        \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 26, 26, 8)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 4)         288       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 4)         16        \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 24, 24, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 22, 22, 4)         144       \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 22, 22, 4)         16        \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 22, 22, 4)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 22, 22, 3)         12        \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 22, 22, 3)         12        \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 22, 22, 3)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 11, 11, 3)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_5 (Conv2D)            (None, 9, 9, 16)          432       \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 9, 9, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 9, 9, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_6 (Conv2D)            (None, 7, 7, 16)          2304      \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 7, 7, 16)          64        \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 7, 7, 16)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 1, 1, 10)          7840      \n",
            "_________________________________________________________________\n",
            "batch_normalization_7 (Batch (None, 1, 1, 10)          40        \n",
            "_________________________________________________________________\n",
            "dropout_7 (Dropout)          (None, 1, 1, 10)          0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 10)                0         \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 11,336\n",
            "Trainable params: 11,214\n",
            "Non-trainable params: 122\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_0RKp7mKtc4",
        "colab_type": "text"
      },
      "source": [
        "Compile the model based on following:\n",
        "\n",
        "* Optimization method: Here we use 'adam'\n",
        "* Kind of loss this method will optimize: Here we use 'categorical_crossentropy'\n",
        "\n",
        "Start training the model:\n",
        "\n",
        "* Batch size: set to 128\n",
        "* Epoch: set to 30"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zp6SuGrL9M3h",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        },
        "outputId": "f6d7bf51-0600-4baf-9887-87f98873b144"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "             optimizer='adam',\n",
        "             metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, Y_train, batch_size=128, nb_epoch=30, validation_data=(X_test, Y_test), verbose=1)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 9s 153us/step - loss: 0.7469 - acc: 0.8044 - val_loss: 0.2977 - val_acc: 0.9640\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 6s 97us/step - loss: 0.4217 - acc: 0.8805 - val_loss: 0.1868 - val_acc: 0.9772\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.3328 - acc: 0.8984 - val_loss: 0.1214 - val_acc: 0.9823\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2822 - acc: 0.9120 - val_loss: 0.0967 - val_acc: 0.9844\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.2549 - acc: 0.9190 - val_loss: 0.0786 - val_acc: 0.9867\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 5s 86us/step - loss: 0.2258 - acc: 0.9274 - val_loss: 0.0697 - val_acc: 0.9852\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.2071 - acc: 0.9331 - val_loss: 0.0561 - val_acc: 0.9879\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1886 - acc: 0.9386 - val_loss: 0.0513 - val_acc: 0.9879\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1773 - acc: 0.9411 - val_loss: 0.0485 - val_acc: 0.9885\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1653 - acc: 0.9438 - val_loss: 0.0444 - val_acc: 0.9879\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1566 - acc: 0.9446 - val_loss: 0.0442 - val_acc: 0.9892\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1511 - acc: 0.9462 - val_loss: 0.0419 - val_acc: 0.9888\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1416 - acc: 0.9481 - val_loss: 0.0356 - val_acc: 0.9913\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1384 - acc: 0.9472 - val_loss: 0.0347 - val_acc: 0.9906\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1344 - acc: 0.9479 - val_loss: 0.0399 - val_acc: 0.9885\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1290 - acc: 0.9497 - val_loss: 0.0307 - val_acc: 0.9909\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1257 - acc: 0.9499 - val_loss: 0.0339 - val_acc: 0.9908\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1253 - acc: 0.9490 - val_loss: 0.0319 - val_acc: 0.9907\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1197 - acc: 0.9516 - val_loss: 0.0313 - val_acc: 0.9906\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 5s 85us/step - loss: 0.1210 - acc: 0.9496 - val_loss: 0.0301 - val_acc: 0.9920\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 6s 93us/step - loss: 0.1210 - acc: 0.9489 - val_loss: 0.0283 - val_acc: 0.9915\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 5s 88us/step - loss: 0.1142 - acc: 0.9525 - val_loss: 0.0278 - val_acc: 0.9919\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1112 - acc: 0.9527 - val_loss: 0.0261 - val_acc: 0.9915\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1122 - acc: 0.9513 - val_loss: 0.0351 - val_acc: 0.9896\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1126 - acc: 0.9511 - val_loss: 0.0332 - val_acc: 0.9903\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1099 - acc: 0.9516 - val_loss: 0.0258 - val_acc: 0.9917\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1109 - acc: 0.9515 - val_loss: 0.0272 - val_acc: 0.9912\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1089 - acc: 0.9517 - val_loss: 0.0271 - val_acc: 0.9918\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 5s 82us/step - loss: 0.1060 - acc: 0.9529 - val_loss: 0.0239 - val_acc: 0.9928\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 5s 81us/step - loss: 0.1074 - acc: 0.9527 - val_loss: 0.0275 - val_acc: 0.9920\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe93fb20860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "40sp66OuwrZ2",
        "colab_type": "text"
      },
      "source": [
        "**Various Channel Size Constraints and the corresponding validation accuracy**\n",
        "\n",
        "![Channel Size Constarint Types](https://rashidmeras.github.io/images/eva/S4_Proposal3_Table1.png)\n",
        "\n",
        "The table above shows various channel size constraint in Block1 and Block2 and correspoding Total parameter and validation accuracy. The Row No.4 indicates the performance of **Network Type4** that we have defined in this *Proposal3*. Comapred to other channel size conatraints Row No.4 has total params that is not too high or too low and has an average performance with respect to the vaildation accuracy.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GR8WNm_F0yux",
        "colab_type": "text"
      },
      "source": [
        "Result:\n",
        "* Total params: **11,336**\n",
        "* Trainable params: 11,214\n",
        "* Non-trainable params: 122\n",
        "\n",
        ">* Score (validation accuracy): **99.28%** at 29th epoch\n",
        "\n",
        "Analysis:\n",
        "> The total number of parameters is **11336** (under 15K) and the validation accuracy is **99.28%** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y2n97OrkLffn",
        "colab_type": "text"
      },
      "source": [
        "###Summary:\n",
        "\n",
        "> 1. In this section we re-analysed the Proposal2: Network Type3 and categorized it into different blocks. Then we constarined the channels of each blocks by reducing it by 1/2 , 1/4 or keeping it the same. Then we introduced **Network Type4** that has reduced channel sizes by 1/4th only in Block1 (which is above the Transition layer) compared to it's predecessor network.\n",
        "\n",
        "> 2. The **Network Type4** that we have defined in this section has **11,336** total params and achieves a validation accuracy of **99.28%**. \n",
        "\n",
        "> In terms of the total params we have already achieved the target of being under 15K now we need to improvise using new techniques to increase the validation accuracy.\n",
        "\n",
        "\n",
        "###Thank you!"
      ]
    }
  ]
}
